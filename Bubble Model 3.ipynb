{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Growth Model\n",
    "\n",
    "The Solow Growth Model, developed by Robert Solow in the 1950s, is a cornerstone of theoretical economics, providing insights into the determinants of economic growth and the role of capital accumulation, labor force, and technological progress.\n",
    "\n",
    "### The Basic Model\n",
    "\n",
    "The Solow model posits that the output of an economy (Y) depends on the amounts of capital (K) and labor (L), along with a constant representing the level of technology (A). The model is often expressed with a production function that has constant returns to scale, typically a Cobb-Douglas production function:\n",
    "\n",
    "$$ Y = A K^\\alpha L^{1-\\alpha} $$\n",
    "\n",
    "where:\n",
    "- \\( Y \\) is the total output (or real GDP) of the economy.\n",
    "- \\( A \\) is a factor productivity term representing technology.\n",
    "- \\( K \\) is the total amount of physical capital in the economy.\n",
    "- \\( L \\) is the total labor force.\n",
    "- \\( \\alpha \\) (0 < α < 1) is the output elasticity of capital, indicating the percentage increase in output resulting from a percentage increase in capital, holding labor constant.\n",
    "\n",
    "### Simplified Assumptions\n",
    "\n",
    "To simplify the analysis, the Solow model often assumes that labor grows at a constant exogenous rate and that savings and investment are a fixed proportion of output. The model shows how these investments affect the capital stock over time, leading to economic growth through capital accumulation. The model also explores the concept of steady-state equilibrium, where the capital stock remains constant over time when depreciation and capital formation are balanced.\n",
    "\n",
    "### Implications\n",
    "\n",
    "One of the key insights from the Solow model is that higher savings rates can lead to a higher level of steady-state capital and output, but not to a higher growth rate in the long run. The long-term growth rate is determined by technological progress, according to the Solow model.\n",
    "\n",
    "The model highlights the importance of technological advances and productivity improvements in sustaining long-term economic growth, beyond just the accumulation of capital and labor inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyman Minsky's Financial Instability Hypothesis and Minsky Moments\n",
    "\n",
    "Hyman Minsky, an American economist, developed the Financial Instability Hypothesis (FIH), which emphasizes the dynamics of financial markets and their role in shaping economic cycles. His theory proposes that financial systems inherently tend toward periods of boom and bust due to the cyclical fluctuations in investor behavior and economic stability.\n",
    "\n",
    "### Financial Instability Hypothesis\n",
    "\n",
    "Minsky's model is built on the observation that during prosperous times, firms and consumers alike tend to increase their debt, becoming progressively riskier in their financial behaviors. This leads to what Minsky identified as three types of borrowers:\n",
    "- **Hedge Borrowers:** Can pay back both the interest and principal from their cash flows.\n",
    "- **Speculative Borrowers:** Can cover the interest but must continually roll over the principal.\n",
    "- **Ponzi Borrowers:** Cannot cover the interest or principal from their operations and must borrow further or sell assets to meet their obligations.\n",
    "\n",
    "As the economy grows, the proportion of speculative and Ponzi borrowers increases, elevating the financial risk within the system.\n",
    "\n",
    "### Minsky Moments\n",
    "\n",
    "A \"Minsky Moment\" is a sudden market collapse that follows a long period of bullish growth, typically caused by the unsustainable accumulation of debt by speculative and Ponzi borrowers. When these borrowers fail to meet their debt obligations due to a downturn or tightening credit conditions, it can lead to a rapid deflation of asset prices, increased volatility, and a retrenchment of economic activity.\n",
    "\n",
    "These moments are characterized by:\n",
    "- Rapid deleveraging\n",
    "- Falling asset prices\n",
    "- Abrupt slowdowns in economic activity\n",
    "\n",
    "### Implications\n",
    "\n",
    "The FIH and the concept of Minsky Moments have become particularly relevant in discussions of economic policy and regulation, especially following the global financial crisis of 2007-2008, which many analysts consider a quintessential Minsky Moment. Minsky's work suggests that to prevent such crises, regulatory measures should focus on curtailing excessive debt accumulation and managing the cyclical expansions of financial behavior.\n",
    "\n",
    "Minsky's insights challenge the traditional views of financial markets always moving toward equilibrium, emphasizing instead the potential for sudden and severe economic instability due to internal market dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preprocessing Overview\n",
    "\n",
    "This project involves analyzing economic and financial data to detect potential asset price bubbles. The data sources include GDP and job openings from governmental sources like the Federal Reserve Economic Data (FRED), the United States Census Bureau (USCB), and the Bureau of Labor Statistics (BLS), as well as stock market indices from various global markets.\n",
    "\n",
    "### File Paths and Initial Data Loading\n",
    "\n",
    "- **File Paths:** The project's data files are stored in a structured directory. The paths are set relative to a base path defined at the beginning of the script.\n",
    "- **Loading Data:** The GDP and country keys data are loaded into pandas dataframes from CSV files. The GDP data includes multiple series, which are filtered based on a selected country.\n",
    "\n",
    "### Specific Column Selection\n",
    "\n",
    "- **Country and Series Identification:** A specific country, in this case, the United States, is selected, and corresponding series IDs for GDP and the stock index are extracted from the country keys data.\n",
    "- **Data Filtering:** The main dataframe is filtered to include only the relevant GDP series and the 'USJO' column, which represents job openings data sourced from the BLS.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "- **Handling Missing Values:** The dataframe is cleaned by dropping any rows with missing values to ensure the integrity of the analyses.\n",
    "- **Date Conversion:** The 'DATE' column is converted to datetime format, which facilitates time-series analysis and merging operations with other time-based data sources.\n",
    "\n",
    "### Function for Data Selection\n",
    "\n",
    "- **`get_observations_before_date_v3`:** This function extracts a specific number of observations from the dataframe before a given date. It ensures that the data is sorted and that only the relevant observations are returned, which is crucial for time-sensitive financial analyses.\n",
    "\n",
    "### Stock Market Data Acquisition\n",
    "\n",
    "- **Stock Index Data:** Stock data for the selected country’s primary index is downloaded using the `yfinance` library, which provides access to historical market data. The data is resampled to a quarterly frequency to match the GDP and job openings data.\n",
    "\n",
    "### Data Merging\n",
    "\n",
    "- **As-Of Merge:** The economic data (GDP and job openings) is merged with the stock market data using a backward as-of merge. This method aligns each stock market observation with the latest available economic data observation as of each date in the stock data.\n",
    "\n",
    "### Data Alignment and Preparation for Analysis\n",
    "\n",
    "- **Trimming and Alignment:** The length of the arrays containing calculated statistics (like GSADF statistics) is adjusted to match the length of the merged dataframe. This involves either trimming excess data or padding with NaNs to account for missing periods.\n",
    "- **Final Merging:** The adjusted GSADF statistics are added to the merged dataframe, creating a comprehensive dataset ready for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.vector_ar.vecm import select_order\n",
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM, coint_johansen\n",
    "from statsmodels.tsa.vector_ar.vecm import VECM\n",
    "import os\n",
    "\n",
    "import mplcyberpunk\n",
    "plt.style.use(\"cyberpunk\")\n",
    "# python 3.9.12 for cyberpunk plot style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "\n",
    "series_list = '/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/Data/Series_List.csv'\n",
    "pop_projections = '/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/Data/US_Census_Pop_Projections.csv'\n",
    "df = pd.read_csv(series_list)\n",
    "countries = pd.read_csv('/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/Data/Country Keys.csv')\n",
    "\n",
    "# Correctly select specific columns\n",
    "country = countries[['Country', 'GDP_SeriesID', 'Stock_index']]\n",
    "# Example: Selecting the GDP_SeriesID for the USA\n",
    "select = 'United States'\n",
    "\n",
    "gdp_series_id = country.loc[country['Country'] == select, 'GDP_SeriesID'].values[0]\n",
    "title = country.loc[country['Country'] == select, 'Country'].values[0]\n",
    "\n",
    "# Assuming 'df' contains columns like 'DATE' and many GDP series\n",
    "df = df[['DATE', gdp_series_id, 'USE' , 'USJO', 'USU', 'USP', 'Lockdowns']]#, 'USJO' ,'USU']]  # Selecting DATE and the specific GDP series column\n",
    "df = df.rename(columns={gdp_series_id: 'GDP'})  # Rename the GDP series column\n",
    "\n",
    "# Clean up data\n",
    "df = df.dropna()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(pop_projections)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observations_before_date_v3(df, date_column, date_str, num_observations):\n",
    "    \"\"\"\n",
    "    Extracts a specified number of observations from a DataFrame immediately before a given date using a date column.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        date_column (str): The name of the column that contains the date values.\n",
    "        date_str (str): The target date in 'YYYY-MM-DD' format.\n",
    "        num_observations (int): The number of observations to retrieve before the date.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A subset of the original DataFrame with the specified number of observations before the date.\n",
    "    \"\"\"\n",
    "    # Convert the date column to datetime if not already\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Sort the DataFrame by the date column to ensure chronological order\n",
    "    df.sort_values(by=date_column, inplace=True)\n",
    "\n",
    "    # Filter the DataFrame to only include rows before the target date\n",
    "    before_date_df = df[df[date_column] < pd.to_datetime(date_str)]\n",
    "\n",
    "    # Select the last 'num_observations' entries from this filtered DataFrame\n",
    "    if len(before_date_df) > num_observations:\n",
    "        df_subset = before_date_df.iloc[-num_observations:]\n",
    "    else:\n",
    "        df_subset = before_date_df  # In case there are fewer rows than 'num_observations'\n",
    "\n",
    "    return df_subset\n",
    "cut_date = '2016-10-31'\n",
    "subset_df = get_observations_before_date_v3(df, 'DATE', cut_date, 295)\n",
    "\n",
    "#df = subset_df\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, pick a stock you would like to forcast from yahoo finance. For the purposes of demonstration we will use quarterly data of the DJIA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stock data from yfinance \n",
    "# ^IXIC Nasdaq\n",
    "# ^GSPC S&P\n",
    "# ^DJA Dow\n",
    "# Brazil Index ^BVSP\n",
    "# India Index  ^BSESN\n",
    "# Germany ^GDAXI\n",
    "# Japan ^N225\n",
    "# Mexico ^MXX\n",
    "ticker = country.loc[country['Country'] == select, 'Stock_index'].values[0]\n",
    "start = '1980-01-01'\n",
    "end = df['DATE'].iloc[-1]\n",
    "data = yf.download(ticker, start, end, interval='3mo')['Close']\n",
    "T = len(data)\n",
    "data_df = data.reset_index()\n",
    "data_df.rename(columns={'Close': 'Price'}, inplace=True)\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge_asof(df.sort_values('DATE'), \n",
    "                          data_df.sort_values('Date'), \n",
    "                          left_on='DATE', \n",
    "                          right_on='Date', \n",
    "                          direction='backward')\n",
    "merged_df = merged_df.dropna()\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = data_df['Price']\n",
    "price \n",
    "T = len(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Explosive Unit Root Analysis Using SADF and GSADF Tests\n",
    "\n",
    "The provided Python code implements the SADF and GSADF tests to detect explosive behavior (unit roots) in time series data. These tests are useful for identifying speculative bubbles or other forms of non-stationarity within economic data, which might indicate periods of excessive price increases followed by abrupt corrections.\n",
    "\n",
    "### Initial Settings\n",
    "\n",
    "- **`r0`**: The initial proportion of the sample used for the minimum window size for the rolling tests. It is set based on a formula that adjusts with the square root of the sample size `T`.\n",
    "- **`swindow0`**: The actual size of the initial window converted from the proportion `r0`.\n",
    "- **`dim`**: The number of test statistics that will be calculated, based on the total sample size `T` minus the size of the initial window `swindow0`.\n",
    "- **`date`**: An array of dates that correspond to the length of the data, assuming quarterly frequency starting from the first index of the merged dataset.\n",
    "\n",
    "### Functions\n",
    "\n",
    "- **`simulate_critical_values(num_simulations, T, r0)`**: This function simulates critical values for the SADF/GSADF tests. It does so by repeatedly generating a unit root process and calculating the ADF statistic for each simulation. The critical values for the 90%, 95%, and 99% confidence levels are then derived from these simulated statistics.\n",
    "\n",
    "### Calculation of SADF and GSADF Statistics\n",
    "\n",
    "- **SADF Calculation**:\n",
    "  - The SADF statistic is computed using a rolling window approach where the ADF test is applied to expanding windows of data starting from the initial window size to the end of the series.\n",
    "  - The maximum ADF statistic from these windows is taken as the SADF statistic, indicating the strongest evidence of a unit root within any window.\n",
    "\n",
    "- **GSADF Calculation**:\n",
    "  - Similar to the SADF, but the GSADF allows for both starting and ending points of the window to vary.\n",
    "  - This test computes the ADF statistic for every possible window combination within the dataset, starting from the initial window size.\n",
    "  - The maximum of these statistics is the GSADF statistic, providing a more flexible test that can detect multiple periods of explosive behavior.\n",
    "\n",
    "### Output\n",
    "\n",
    "- The code outputs the calculated SADF and GSADF statistics and compares them against the critical values derived from the simulations. It also prints the length of the GSADF statistics array and the corresponding dates, which helps in identifying the specific time periods of potential explosive behavior.\n",
    "\n",
    "### Usage\n",
    "\n",
    "- The code is set up to analyze a series (likely stock prices or similar financial data) stored in the variable `price`.\n",
    "- Users can adjust the number of simulations and the sample size `T` as needed based on their specific dataset and the desired accuracy of the test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial settings for the SADF test window\n",
    "r0 = 0.01 + 1.8 / np.sqrt(T)\n",
    "swindow0 = int(np.floor(r0 * T))\n",
    "dim = T - swindow0 + 1\n",
    "date = pd.date_range(start= merged_df.index[0], periods=T, freq='Q-DEC')\n",
    "\n",
    "def simulate_critical_values(num_simulations, T, r0):\n",
    "    critical_values = []\n",
    "    for _ in range(num_simulations):\n",
    "        # Generate a unit root process\n",
    "        series = np.random.normal(size=T).cumsum()\n",
    "        # Placeholder for SADF/GSADF calculation: this should ideally be replaced\n",
    "        # with a function that calculates the SADF/GSADF statistic.\n",
    "        stat = adfuller(series, maxlag=int(r0*T), regression='c', autolag=None)[0]\n",
    "        critical_values.append(stat)\n",
    "    \n",
    "    # Determine the 90%, 95%, and 99% critical values\n",
    "    critical_values = np.percentile(critical_values, [90, 95, 99])\n",
    "    return critical_values\n",
    "\n",
    "# Example usage\n",
    "num_simulations = 2000\n",
    "\n",
    "critical_values = simulate_critical_values(num_simulations, T, r0)\n",
    "print(\"Critical values at 90%, 95%, 99%:\", critical_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the SADF statistic\n",
    "dateS = date[swindow0:]\n",
    "\n",
    "badfs = np.zeros(dim)\n",
    "for i in range(swindow0, T):\n",
    "    result = adfuller(price[:i+1], maxlag=2, regression='c', autolag=None)\n",
    "    badfs[i - swindow0] = result[0]\n",
    "sadf = np.max(badfs)\n",
    "\n",
    "print('The SADF statistic:', sadf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Generalized SADF statistic\n",
    "bsadfs = np.zeros(dim)\n",
    "for r2 in range(swindow0, T):\n",
    "    dim0 = r2 - swindow0 + 1\n",
    "    rwadft = np.zeros(dim0)\n",
    "    for r1 in range(dim0):\n",
    "        result = adfuller(price[r1:r2+1], maxlag=2, regression='c', autolag=None)\n",
    "        rwadft[r1] = result[0]\n",
    "    bsadfs[r2 - swindow0] = np.max(rwadft)\n",
    "\n",
    "gsadf = np.max(bsadfs)\n",
    "\n",
    "print('The GSADF statistic:', gsadf)\n",
    "print('The critical values:', critical_values)\n",
    "bsadfs = bsadfs[:-1]\n",
    "len(bsadfs),len(dateS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming bsadfs, merged_df, and other necessary variables are already defined\n",
    "# Print current lengths to understand the situation\n",
    "print(\"Length of dateS:\", len(dateS))\n",
    "print(\"Length of bsadfs before trimming:\", len(bsadfs))\n",
    "print(\"Length of badfs:\", len(badfs))\n",
    "\n",
    "# Trim bsadfs if it's longer than merged_df\n",
    "if len(bsadfs) > len(merged_df):\n",
    "    bsadfs = bsadfs[-len(merged_df):]  # Trim from the beginning to match the end\n",
    "elif len(bsadfs) < len(merged_df):\n",
    "    # Calculate the number of missing periods\n",
    "    missing_periods = len(merged_df) - len(bsadfs)\n",
    "    # Create an array of NaNs for the missing periods\n",
    "    initial_nans = np.full(missing_periods, np.nan)\n",
    "    # Concatenate the NaNs with the bsadfs array to align with the end of merged_df\n",
    "    bsadfs = np.concatenate((initial_nans, bsadfs))\n",
    "\n",
    "# Add the bsadfs array to the DataFrame\n",
    "merged_df['GSADF_Stat'] = bsadfs\n",
    "\n",
    "# Optional: Drop rows with NaNs in the 'GSADF_Stat' if needed\n",
    "merged_df = merged_df.dropna(subset=['GSADF_Stat'])\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['Date'])\n",
    "\n",
    "# Display the DataFrame to verify the column has been removed\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_transformations(df, columns):\n",
    "    \"\"\"\n",
    "    Add log, first difference, and log difference transformations to specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to modify.\n",
    "    columns (list of str): List of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The modified DataFrame with new columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        # Calculate log of the column\n",
    "        df[f'Log_{col}'] = np.log(df[col])\n",
    "\n",
    "        # Calculate first difference of the column\n",
    "        df[f'{col}_Diff'] = df[col].diff()\n",
    "\n",
    "        # Calculate first difference of the log of the column\n",
    "        df[f'Log_{col}_Diff'] = df[f'Log_{col}'].diff()\n",
    "\n",
    "    # Drop the initial NaN values resulting from differencing\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define the columns you want to transform\n",
    "columns_to_transform = ['GDP', 'Price', 'USE', 'USJO', 'USU', 'USP']\n",
    "\n",
    "# Apply the transformations\n",
    "merged_df.set_index('DATE', inplace=True)\n",
    "merged_df = add_transformations(merged_df, columns_to_transform)\n",
    "\n",
    "# Display the resulting merged DataFrame\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/Data/vecm_data.csv', index=True)  # creating csv of this data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color variables\n",
    "color_job_openings = 'cyan'\n",
    "color_bsadf_stat = 'red'\n",
    "color_stock_index = 'limegreen'\n",
    "color_gdp = 'yellow'\n",
    "color_population = 'blue'\n",
    "color_unemployment = 'magenta'\n",
    "color_employment = 'orange'\n",
    "axis_color = 'white' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Log_USE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.plot(merged_df.index, merged_df['Log_USE'], label='Log USE', marker='o', color=color_employment)\n",
    "plt.plot(merged_df.index, merged_df['Log_USP'], label='Log USP', marker='o', color=color_population)\n",
    "\n",
    "plt.title(f'{select} Job Openings and Unemployment', color=axis_color, fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Logarithmic Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}Pop_Growth & Employment.jpeg', format='jpeg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Checking Cointegration, Seasonality, and Stationarity in VECM Specification\n",
    "\n",
    "When specifying a Vector Error Correction Model (VECM), it is crucial to assess cointegration, seasonality, and stationarity of the involved time series. These checks are fundamental for ensuring the validity of the model, the accuracy of the inferences drawn from it, and the effectiveness of the model in capturing the underlying economic dynamics.\n",
    "\n",
    "### 1. Stationarity\n",
    "\n",
    "- **Definition:** A stationary time series has a constant mean, variance, and autocorrelation structure over time, meaning its statistical properties do not change over time.\n",
    "- **Importance in VECM:**\n",
    "  - **Model Stability and Reliability:** Non-stationary data can lead to spurious results in regression-type models, which assume stationary inputs. VECM requires that all series included in the model are integrated of the same order, typically I(1), which means they become stationary after first differencing.\n",
    "  - **Pre-condition for Cointegration:** Only non-stationary series that are integrated of the same order can be tested for cointegration, a key component in the VECM framework.\n",
    "\n",
    "### 2. Cointegration\n",
    "\n",
    "- **Definition:** Cointegration occurs when a linear combination of two or more non-stationary series is itself stationary. This implies a long-term equilibrium relationship among the series despite short-term deviations.\n",
    "- **Importance in VECM:**\n",
    "  - **Capturing Long-Term Relationships:** Cointegration indicates that the variables share a common stochastic trend and are bound by an equilibrium relationship, even though they may individually wander away from equilibrium.\n",
    "  - **Error Correction Term (ECT):** VECM includes an error correction term derived from the cointegration equation, which adjusts the short-term dynamics of the dependent variables to correct for deviations from this long-term equilibrium.\n",
    "\n",
    "### 3. Seasonality\n",
    "\n",
    "- **Definition:** Seasonality refers to periodic fluctuations in time series data that occur at regular intervals, such as quarterly, monthly, or yearly.\n",
    "- **Importance in VECM:**\n",
    "  - **Model Accuracy:** Ignoring seasonal variations can lead to misinterpretations of trends and cycles in the data. Seasonal adjustments ensure that the model captures true underlying trends rather than seasonal effects.\n",
    "  - **Improved Forecasting:** Correctly modeling seasonality improves the model's predictive performance, particularly for economic and financial data, which often exhibit strong seasonal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_adf_test(series, regression_type='ct'):\n",
    "    \"\"\"\n",
    "    Perform an Augmented Dickey-Fuller test on a given time series.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): The time series on which to perform the ADF test.\n",
    "        regression_type (str): The type of regression ('c' for constant, 'ct' for constant and trend, 'ctt' for constant, and linear and quadratic trend, 'nc' for no constant, no trend).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    result = adfuller(series, regression=regression_type)\n",
    "    print(f\"ADF Statistic for {series.name}: {result[0]}\")\n",
    "    print(f\"p-value for {series.name}: {result[1]}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f'\\t{key}: {value:.3f}')\n",
    "\n",
    "    if result[0] < result[4]['10%']:\n",
    "        print(f\"Reject the null hypothesis - the {series.name} series is stationary at the 90% confidence level.\")\n",
    "    else:\n",
    "        print(f\"Fail to reject the null hypothesis - the {series.name} series is not stationary at the 90% confidence level.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in merged_df.columns:\n",
    "    result = perform_adf_test(merged_df[column], regression_type='ct')\n",
    "    print(f'ADF Test for {column}:\\n{result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Correlogram for Log Stock Index Price\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['Log_Price'], lags=40, zero=False, title='Autocorrelation of Log Stock Index Price', color=color_stock_index)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# Plot Correlogram for Log GDP\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['Log_GDP'], lags=40, zero=False, title='Autocorrelation of Log National GDP', color=color_gdp)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# Plot Correlogram for Log US Job Openings\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['Log_USJO'], lags=40, zero=False, title='Autocorrelation of Log National Job Openings', color=color_job_openings)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# Plot Correlogram for Log Unemployment\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['Log_USU'], lags=40, zero=False, title='Autocorrelation of Stock Index Price', color=color_unemployment)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# Plot Correlogram for Log Unemployment\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['Log_USP'], lags=40, zero=False, title='Autocorrelation of Population', color=color_population)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "# Plot Correlogram for GSADF Statistic\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_acf(merged_df['GSADF_Stat'], lags=40, zero=False, title='Autocorrelation of GSADF Statistic', color=color_bsadf_stat)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the plot with specified figure size\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax1 = plt.gca()  # Get the current axes instance\n",
    "\n",
    "# Plotting Job Openings on the left y-axis\n",
    "ax1.plot(merged_df.index, merged_df['Log_USJO'], label='Log of US Job Openings', color=color_job_openings)\n",
    "ax1.plot(merged_df.index, merged_df['Log_USU'], label='Log of US Unemployment', color=color_unemployment)\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Log of US Job Openings', color=color_job_openings)\n",
    "ax1.tick_params(axis='y', colors=axis_color)\n",
    "ax1.grid(False)  # Disable gridlines for the primary axis\n",
    "\n",
    "# Create a second y-axis for the BSADF Statistic data\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged_df.index, merged_df['GSADF_Stat'], label='Test Statistic', color=color_bsadf_stat)\n",
    "ax2.set_ylabel('BSADF Statistic', color=color_bsadf_stat)\n",
    "ax2.tick_params(axis='y', colors=axis_color)\n",
    "ax2.grid(False)  # Disable gridlines for the secondary axis\n",
    "\n",
    "# Title and legend handling\n",
    "plt.title(f'{select} Employment Statistics and Stock Market Bubbles over Time', color=axis_color, fontsize=18, fontweight='bold')\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust location as needed\n",
    "\n",
    "# Save the plot to the specified directory and file\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}_Unemployment_Test_Stat.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots with the desired size\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot the Price data on the primary y-axis\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Stock Index', color=color_stock_index)\n",
    "ax1.plot(merged_df.index, merged_df['Log_Price'], color=color_stock_index, label='Log Stock Index')\n",
    "ax1.tick_params(axis='y', labelcolor=axis_color)\n",
    "ax1.grid(False)  # Disable gridlines for the primary axis\n",
    "\n",
    "# Create a second y-axis for the GDP and US Prices data\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('GDP and US Prices', color=color_gdp)  # We already handled the x-label with ax1\n",
    "ax2.plot(merged_df.index, merged_df['Log_GDP'], color=color_gdp, label='Log GDP')\n",
    "ax2.plot(merged_df.index, merged_df['Log_USP'], color=color_population, linestyle='--', label='Log US Population')  # Corrected label\n",
    "ax2.tick_params(axis='y', labelcolor=axis_color)\n",
    "ax2.grid(False)  # Disable gridlines for the secondary axis\n",
    "\n",
    "# Title and layout\n",
    "plt.title('Stock Index, National GDP, and US Prices Over Time')\n",
    "fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "\n",
    "# Add legend to differentiate the data series\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust location as needed\n",
    "plt.title(f'{select} Capital Stock, Output and Population Growth over Time', color=axis_color, fontsize=18, fontweight='bold')\n",
    "\n",
    "# Save the plot to the specified directory and file\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}_Index_Price_GDP_USP.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize plotting function\n",
    "def customize_plot(result, title, color):\n",
    "    fig, axes = plt.subplots(ncols=1, nrows=4, sharex=True, figsize=(12, 8))\n",
    "    axes[0].plot(result.observed, color=color)\n",
    "    axes[0].set_title('Observed')\n",
    "    \n",
    "    axes[1].plot(result.trend, color=color)\n",
    "    axes[1].set_title('Trend')\n",
    "    \n",
    "    axes[2].plot(result.seasonal, color=color)\n",
    "    axes[2].set_title('Seasonal')\n",
    "    \n",
    "    axes[3].plot(result.resid, color=color)\n",
    "    axes[3].set_title('Residual')\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.tick_params(axis='x', colors=axis_color)\n",
    "        ax.tick_params(axis='y', colors=axis_color)\n",
    "\n",
    "    plt.suptitle(title, color=axis_color)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Applying the function to each series\n",
    "# Decompose and plot Stock Index data\n",
    "result_price = seasonal_decompose(merged_df['Price'], model='additive', period=4)  \n",
    "customize_plot(result_price, 'Stock Index Decomposition', color_stock_index)\n",
    "\n",
    "# Decompose and plot GDP data\n",
    "result_gdp = seasonal_decompose(merged_df['GDP'], model='additive', period=4)  \n",
    "customize_plot(result_gdp, 'GDP Decomposition', color_gdp)\n",
    "\n",
    "# Decompose and plot Unemployment data\n",
    "result_job_openings = seasonal_decompose(merged_df['USU'], model='additive', period=4)  \n",
    "customize_plot(result_job_openings, 'Unemployment Decomposition', color_unemployment)\n",
    "# Decompose and plot Job Openings data\n",
    "result_job_openings = seasonal_decompose(merged_df['USJO'], model='additive', period=4)  \n",
    "customize_plot(result_job_openings, 'Job Openings Decomposition', color_job_openings)\n",
    "\n",
    "# Decompose and plot Population data\n",
    "result_population = seasonal_decompose(merged_df['USP'], model='additive', period=4)  \n",
    "customize_plot(result_population, 'Population Decomposition', color_population)\n",
    "# Decompose and plot GSADF data\n",
    "result_gsadf = seasonal_decompose(merged_df['GSADF_Stat'], model='additive', period=4)  \n",
    "customize_plot(result_gsadf, 'GSADF Stat Decomposition', color_bsadf_stat)\n",
    "\n",
    "merged_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Error Correction Model (VECM) with Explosive Unit Root, Job Openings, Stock Market Growth, and GDP\n",
    "\n",
    "The Vector Error Correction Model (VECM) is a multivariate statistical model used to analyze the long-term and short-term dynamics among several non-stationary time series variables that are cointegrated. In this model, we include variables such as an explosive unit root, job openings, stock market growth, and GDP to understand their interdependencies and the adjustments toward equilibrium over time.\n",
    "\n",
    "### Overview of the VECM\n",
    "\n",
    "A VECM is particularly useful when dealing with non-stationary data series that exhibit a long-run equilibrium relationship, as suggested by cointegration tests. The inclusion of the error correction term allows the model to specify both the short-term dynamics and adjustments needed to return to equilibrium after shocks.\n",
    "\n",
    "### Components of the Model\n",
    "\n",
    "- **Explosive Unit Root:** This component typically represents a variable that shows persistent exponential growth or decline, diverging from a stable path. In financial data, this might be related to speculative bubbles or severe recessive movements.\n",
    "- **Job Openings (USJO):** This measures the labor demand and is an indicator of economic health. Fluctuations in job openings can be both a cause and effect of economic cycles.\n",
    "- **Stock Market Growth:** Often a leading indicator of economic health, as it reflects investor expectations about future corporate earnings and macroeconomic trends.\n",
    "- **GDP (Gross Domestic Product):** Represents the total economic output and is a crucial measure of overall economic activity and health.\n",
    "\n",
    "### Functioning of the VECM\n",
    "\n",
    "In the VECM:\n",
    "- The **long-term relationship** is captured by the cointegration among the variables, indicating that despite short-term fluctuations, the variables move together over time towards an equilibrium.\n",
    "- The **short-term dynamics** are detailed by the coefficients of the lagged differences of the variables, indicating how each variable adjusts to changes in the others in the short term.\n",
    "- The **error correction term** (ECT), derived from the deviation from the long-run equilibrium in the previous period, measures the speed and direction of adjustment back towards the equilibrium.\n",
    "\n",
    "### Applications and Implications\n",
    "\n",
    "This model can be particularly insightful for policymakers and investors as it provides:\n",
    "- Insights into how quickly economic variables return to stability after a shock, which is crucial for monetary and fiscal policy decisions.\n",
    "- Understanding the impact of stock market fluctuations and job market dynamics on overall economic growth.\n",
    "- Early warnings of economic overheating or underperformance through the explosive unit root component, guiding preemptive actions.\n",
    "\n",
    "By analyzing the interactions and adjustments between these critical economic indicators, a VECM helps in predicting future economic conditions and designing informed economic policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Determine the optimal lag order\n",
    "data = merged_df[['Log_Price', 'Log_GDP', 'Log_USJO','Log_USU','GSADF_Stat','Log_USP']]\n",
    "\n",
    "lag_order = select_order(data=data, maxlags=20, deterministic=\"ci\")\n",
    "print(lag_order.summary())\n",
    "optimal_lags = lag_order.fpe\n",
    "# Override if still overfit.  Lag length is in large part a judgement call\n",
    "optimal_lags =  7\n",
    "optimal_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Johansen cointegration test\n",
    "data = merged_df[['Log_Price', 'Log_GDP', 'Log_USJO','Log_USU','GSADF_Stat','Log_USP', 'Lockdowns']]\n",
    "\n",
    "result = coint_johansen(data[['Log_Price', 'Log_GDP', 'Log_USJO','Log_USU','GSADF_Stat','Log_USP']], det_order=0, k_ar_diff= optimal_lags)\n",
    "\n",
    "# Print the Trace Statistics and Critical Values\n",
    "print('Trace Statistics:', result.lr1)\n",
    "print('Critical Values (90%, 95%, 99%):', result.cvt)\n",
    "\n",
    "# Interpret and print whether cointegration is found\n",
    "cointegration_found = False  # A flag to keep track of cointegration status\n",
    "for i, trace_stat in enumerate(result.lr1):\n",
    "    if trace_stat > result.cvt[i][0]:  # Index 1 is for the 95% confidence level\n",
    "        cointegration_found = True\n",
    "        print(f\"At the 90% confidence level, we reject the null hypothesis of at most {i} cointegrating relations; indicating cointegration.\")\n",
    "    else:\n",
    "        print(f\"At the 90% confidence level, we fail to reject the null hypothesis of at most {i} cointegrating relations; no cointegration indicated.\")\n",
    "\n",
    "# Print overall result based on tests\n",
    "if cointegration_found:\n",
    "    print(\"The series exhibit a long-run cointegrating relationship.\")\n",
    "else:\n",
    "    print(\"No long-run cointegrating relationship is evident among the series.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_test = select_coint_rank(data[['Log_Price', 'Log_GDP', 'Log_USJO','Log_USU','GSADF_Stat','Log_USP']], 0, optimal_lags, method=\"trace\",\n",
    "                              signif=0.10)\n",
    "rank_test.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results of the `select_rank` Function\n",
    "\n",
    "The `select_rank` function is an essential tool in time series analysis, especially when setting up a Vector Error Correction Model (VECM). This function helps in determining the number of cointegrating relationships among a set of non-stationary variables. Here's a breakdown of what the results mean and how they are interpreted:\n",
    "\n",
    "### Purpose of `select_rank`\n",
    "\n",
    "- **Identify Cointegration Rank:** The main purpose of the `select_rank` function is to identify the rank of cointegration among the variables in a given dataset. The cointegration rank specifies the number of linearly independent vectors that combine the integrated variables into stationary processes.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "- **Test Setup:** The function typically utilizes tests like the Johansen cointegration test, which examines the null hypothesis of `r` cointegrating relationships against the alternative of more cointegrating vectors up to the number of included variables.\n",
    "- **Statistical Methods:** It involves methods such as trace statistics and maximum eigenvalue statistics, which help in determining the presence and number of cointegrating relationships.\n",
    "\n",
    "### Results Interpretation\n",
    "\n",
    "1. **Number of Cointegrating Vectors (Rank)**\n",
    "   - The function outputs the estimated number of cointegrating relationships. This number tells us how many independent long-term relationships exist among the variables.\n",
    "   - **Example:** If `select_rank` returns a cointegration rank of 2 for a system of 5 variables, it implies there are two unique long-term equilibrium relationships binding these variables together.\n",
    "\n",
    "2. **Statistical Values**\n",
    "   - **Trace Statistic and Maximum Eigenvalue:** These are the key outputs used to determine the cointegration rank. Each statistic tests for additional cointegrating vectors.\n",
    "   - The results typically include critical values or p-values, which help in deciding whether to reject the null hypothesis of `r` cointegrating vectors.\n",
    "\n",
    "3. **Model Specification**\n",
    "   - Based on the rank, the error correction terms in the VECM are specified. Each cointegrating relationship will contribute one error correction term to the model, guiding how quickly variables adjust to restore equilibrium after a shock.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round up the last three digits after dividing by 1000\n",
    "def round_up_last_three(x):\n",
    "    return np.ceil(x / 1000)\n",
    "\n",
    "# Apply the function to each of the projection columns correctly\n",
    "df1['US_Proj-lo'] = round_up_last_three(df1['US_Proj-lo']).astype(int)\n",
    "df1['US_Proj-0'] = round_up_last_three(df1['US_Proj-0']).astype(int)\n",
    "df1['US_Proj-hi'] = round_up_last_three(df1['US_Proj-hi']).astype(int)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Date' as the index if it's not already\n",
    "df1.index = pd.to_datetime(df1.index, format='%Y')\n",
    "df1 = df1[2:]\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 5))  # Set the figure size for better visibility\n",
    "\n",
    "# First Plot: Low Migration Scenario\n",
    "plt.subplot(3, 1, 1)  # This means 3 rows, 1 column, first plot\n",
    "plt.plot(df1['US_Proj-lo'], label='Low Migration Scenario', color='blue')\n",
    "plt.title('Low Migration Scenario')\n",
    "plt.ylabel('Population')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(200000, 450000)  # Set uniform y-axis limits\n",
    "\n",
    "# Second Plot: High Migration Scenario\n",
    "plt.subplot(3, 1, 2)  # This means 3 rows, 1 column, second plot\n",
    "plt.plot(df1['US_Proj-hi'], label='High Migration Scenario', color='green')\n",
    "plt.title('High Migration Scenario')\n",
    "plt.ylabel('Population')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(200000, 450000)  # Set uniform y-axis limits\n",
    "\n",
    "# Third Plot: Zero Migration Scenario\n",
    "plt.subplot(3, 1, 3)  # This means 3 rows, 1 column, third plot\n",
    "plt.plot(df1['US_Proj-0'], label='Zero Migration Scenario', color='red')\n",
    "plt.title('Zero Migration Scenario')\n",
    "plt.xlabel('Date')  # Only adding x-label to the bottom plot for clarity\n",
    "plt.ylabel('Population')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(200000, 450000)  # Set uniform y-axis limits\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/Pop_Growth_Estimates.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df1 has the 'Date' index as datetime at the start of each year\n",
    "df1.index = pd.to_datetime(df1.index, format='%Y')\n",
    "\n",
    "# Resample the DataFrame to quarterly, starting from the beginning of the year\n",
    "pop_estimate_df = df1.resample('QS').asfreq()  # 'QS' stands for Quarter Start\n",
    "\n",
    "# Apply interpolation\n",
    "pop_estimate_df.interpolate(method='polynomial', order=3, inplace=True)\n",
    "\n",
    "pop_estimate_df = np.log(pop_estimate_df)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(pop_estimate_df) # Show the last few rows to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and fit the VECM model\n",
    "vecm_model = VECM(endog = data[['Log_Price','Log_GDP','Log_USJO','Log_USU','GSADF_Stat']], \n",
    "                  exog_coint = data[['Log_USP']], \n",
    "                  k_ar_diff= optimal_lags, \n",
    "                  coint_rank=5, \n",
    "                  deterministic= 'ci')\n",
    "vecm_result = vecm_model.fit()\n",
    "vecm_result.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting GDP and Stock Market Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_periods = 200\n",
    "pop_estimate_df = pop_estimate_df.iloc[:num_periods]\n",
    "pop_estimate_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_periods = 200\n",
    "pop_estimate_df = pop_estimate_df.iloc[:num_periods]\n",
    "\n",
    "# Assuming `exog_forecasts_df` contains the exogenous forecasts with columns\n",
    "# 'Log_USP_Forecast1', 'Log_USP_Forecast2', 'Log_USP_Forecast3'\n",
    "pop_estimate_df['Log_USP'] = pop_estimate_df[['US_Proj-hi']]\n",
    "pop_estimate = ['High Migration'] # adjust name of graphs\n",
    "# Here we will just use the first column as an example\n",
    "exog_coint_fc = pop_estimate_df[['Log_USP']].values\n",
    "\n",
    "pop_estimate_df['Lockdowns'] = 0\n",
    "\n",
    "exog_fc = pop_estimate_df['Lockdowns']\n",
    "\n",
    "# Generate the forecasted data using the predict function\n",
    "forecast_df = vecm_result.predict(steps=num_periods, exog_coint_fc=exog_coint_fc)\n",
    "\n",
    "# Plotting the forecast\n",
    "forecast_df = pd.DataFrame(forecast_df, columns=data[['Log_Price','Log_GDP','Log_USJO','Log_USU','GSADF_Stat']].columns, index=pop_estimate_df.index)\n",
    "\n",
    "forecast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Impulse Response Function (IRF) Plots from VECM\n",
    "\n",
    "Impulse Response Function (IRF) plots are a crucial tool in time series analysis, particularly when using Vector Error Correction Models (VECM) to understand the dynamic interactions between variables. These plots illustrate how a shock to one variable affects other variables in the system over time. Here’s an explanation of how to interpret these plots and understand their implications.\n",
    "\n",
    "### What are IRF Plots?\n",
    "\n",
    "- **Definition:** IRF plots show the reaction of dependent variables in a VECM to one-time shocks in each of the other variables. They help in visualizing the time path of these effects and the gradual adjustment of the variables towards equilibrium.\n",
    "- **Purpose:** The primary purpose of generating IRF plots in a VECM framework is to analyze the effect of changes (or shocks) in one variable on the entire system over a specified number of periods.\n",
    "\n",
    "### Generating IRF Plots\n",
    "\n",
    "- **Procedure:** In the provided code snippet, the impulse response function is computed for a given number of periods (`num_periods = 200`). This function simulates the response of all endogenous variables in the model to a shock in each variable, one at a time.\n",
    "- **Visualization:** The method `ir.plot(plot_stderr=False)` generates a series of line plots. Each plot represents the response of one variable to a unit shock in each of the other variables, including itself, over the 200 periods.\n",
    "- **File Saving:** The plots are then saved as JPEG images in a specified directory, allowing for further analysis or presentation.\n",
    "\n",
    "### Interpretation of IRF Plots\n",
    "\n",
    "1. **Response Over Time:**\n",
    "   - Each line on the plot shows how a particular variable reacts over time after the shock. The x-axis represents time in periods after the shock, and the y-axis shows the magnitude of the variable's response.\n",
    "   - Positive values indicate an increase in the variable relative to its baseline value due to the shock, while negative values indicate a decrease.\n",
    "\n",
    "2. **Speed and Magnitude of Adjustment:**\n",
    "   - The plots provide insights into how quickly and significantly variables adjust in response to shocks. Rapid declines or increases followed by stabilization suggest quick adjustments to shocks.\n",
    "   - Large swings or sustained deviations from zero can indicate high sensitivity to shocks and potentially longer-term impacts on the variable.\n",
    "\n",
    "3. **Economic Implications:**\n",
    "   - By observing the IRF plots, analysts can infer the stability and resilience of the economic system or market under study. For example, a prolonged or significant response in GDP or stock prices to a policy change or economic event might signal underlying vulnerabilities or strong dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = vecm_result.irf(periods=num_periods)\n",
    "fig = ir.plot(plot_stderr=False)\n",
    "\n",
    "# Adjust the font size of the labels\n",
    "for ax in fig.axes:\n",
    "    ax.title.set_fontsize(10)  # Adjust title font size\n",
    "    ax.xaxis.label.set_fontsize(8)  # Adjust x-axis label font size\n",
    "    ax.yaxis.label.set_fontsize(8)  # Adjust y-axis label font size\n",
    "    ax.tick_params(axis='both', which='major', labelsize=8)  # Adjust tick labels font size\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "filename = f'{ticker}_irf.jpg'\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select} {pop_estimate} IRF.jpeg', format='jpeg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast Level Calculation:\n",
    "\n",
    "add the forecasted differences back to the last observed level, making the output interpretable in the original data's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the forecasted data with the observed data\n",
    "merged_df = pd.concat([data, forecast_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "\n",
    "# Plotting all logarithmic data on the primary left y-axis\n",
    "ax1.set_xlabel('Date', style='italic', color=axis_color, fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Logarithmic Values', fontsize=12, fontweight='bold')\n",
    "ax1.plot(merged_df.index, merged_df['Log_GDP'], color=color_gdp, label='GDP (Log)')\n",
    "ax1.plot(merged_df.index, merged_df['Log_Price'], color=color_stock_index, label='DJIA Price (Log)')\n",
    "ax1.plot(merged_df.index, merged_df['Log_USJO'], color=color_job_openings, label='Job Openings (Log)')\n",
    "ax1.plot(merged_df.index, merged_df['Log_USU'], color=color_unemployment, label='Unemployment (Log)')\n",
    "ax1.plot(merged_df.index, merged_df['Log_USP'], color=color_population, label='Population (Log)')  # New population line\n",
    "\n",
    "# Optional: Forecast data plotting\n",
    "ax1.plot(forecast_df.index, forecast_df['Log_GDP'], linestyle='-', color=color_gdp, label='GDP (Log)')\n",
    "ax1.plot(forecast_df.index, forecast_df['Log_Price'], linestyle='-', color=color_stock_index, label='DJIA Price (Log)')\n",
    "ax1.plot(forecast_df.index, forecast_df['Log_USJO'], linestyle='-', color=color_job_openings, label='Job Openings (Log)')\n",
    "ax1.plot(forecast_df.index, forecast_df['Log_USU'], linestyle='-', color=color_unemployment, label='Unemployment (Log)')\n",
    "ax1.plot(forecast_df.index, pop_estimate_df['Log_USP'], linestyle='-', color=color_population, label='Population (Log)') \n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor=axis_color, labelsize=10)\n",
    "ax1.grid(False)  # Disable gridlines for this axis\n",
    "\n",
    "# Create a second y-axis for the GSADF Statistic data\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('GSADF Statistic', fontsize=12, fontweight='bold')\n",
    "ax2.plot(merged_df.index, merged_df['GSADF_Stat'], color=color_bsadf_stat, label='Explosivity Test Statistic')\n",
    "ax2.plot(forecast_df.index, forecast_df['GSADF_Stat'], linestyle='-', color=color_bsadf_stat, label='Explosivity Test Statistic')\n",
    "ax2.set_ylabel('Explosivity Test Statistic', fontsize=12, fontweight='bold')\n",
    "ax2.grid(False)  # Disable gridlines for this axis\n",
    "\n",
    "# Adding a vertical line for the last observed date\n",
    "last_observed_date = forecast_df.index[0]  # Assuming forecast_df.index contains the dates\n",
    "plt.axvline(x=last_observed_date, color='black', linestyle='--', linewidth=2, label='Last Observed Date')\n",
    "\n",
    "ax2.axhline(y=critical_values[1], color='black', linestyle='--', linewidth=2, label='Explosive Unit Root Critical Value')\n",
    "\n",
    "# Title and legend handling\n",
    "plt.title(f'{select} Economic Indicators and Stock Market Bubbles over Time', color=axis_color, fontsize=18, fontweight='bold')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Collecting handles and labels from both axes for a unified legend, avoiding duplicates\n",
    "handles, labels = [], []\n",
    "for ax in [ax1, ax2]:\n",
    "    for handle, label in zip(*ax.get_legend_handles_labels()):\n",
    "        if label not in labels:\n",
    "            handles.append(handle)\n",
    "            labels.append(label)\n",
    "ax1.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.25,1.01), frameon = True)\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}_{pop_estimate}_Model.jpeg', format='jpeg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = len(merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot with specified figure size\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax1 = plt.gca()  # Get the current axes instance\n",
    "\n",
    "# Plotting Job Openings on the left y-axis\n",
    "ax1.plot(merged_df.index, merged_df['Log_USJO'], label='Log of US Job Openings', color=color_job_openings)\n",
    "ax1.plot(merged_df.index, merged_df['Log_USU'], label='Log of US Unemployment', color=color_unemployment)\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Logarithmic Values', fontsize=12, fontweight='bold')\n",
    "ax1.tick_params(axis='y', colors=axis_color)\n",
    "ax1.grid(False)  # Disable gridlines for the primary axis\n",
    "\n",
    "# Create a second y-axis for the BSADF Statistic data\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(merged_df.index, merged_df['GSADF_Stat'], label='Explosivity Test Statistic', color=color_bsadf_stat)\n",
    "ax2.set_ylabel('Explosivity Test Statistic', fontsize=12, fontweight='bold')\n",
    "ax2.tick_params(axis='y', colors=axis_color)\n",
    "ax2.grid(False)  # Disable gridlines for the secondary axis\n",
    "\n",
    "\n",
    "# Title and legend handling\n",
    "plt.title('Employment Statistics and Stock Market Bubbles over Time', color=axis_color, fontsize=18, fontweight='bold')\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper left')  # Adjust location as needed\n",
    "\n",
    "# Save the plot to the specified directory and file\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}_{pop_estimate} Unemployment_Test_Stat_F.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots with the desired size\n",
    "\n",
    "# Create a figure and a set of subplots with the desired size\n",
    "fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot the Price data on the primary y-axis\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Capital Stock', fontsize=12, fontweight='bold', color=color_stock_index)\n",
    "line1, = ax1.plot(merged_df.index, merged_df['Log_Price'], color=color_stock_index, label='Dow Jones Industrial Average Prive')\n",
    "ax1.tick_params(axis='y', labelcolor=axis_color)\n",
    "ax1.grid(False)  # Disable gridlines for the primary axis\n",
    "\n",
    "# Create a second y-axis for the GDP data\n",
    "ax2 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('Output', fontsize=12, fontweight='bold', color=color_gdp)\n",
    "line2, = ax2.plot(merged_df.index, merged_df['Log_GDP'], color=color_gdp, label='Gross Domestic Product')\n",
    "ax2.tick_params(axis='y', labelcolor=axis_color)\n",
    "ax2.grid(False)  # Disable gridlines for the secondary axis\n",
    "\n",
    "ax3 = ax1.twinx()  # Instantiate a second axes that shares the same x-axis\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Offset the third axis\n",
    "ax3.set_ylabel('Explosivity Test Statistic', fontsize=12, fontweight='bold', color=color_bsadf_stat)\n",
    "line3, = ax3.plot(merged_df.index, merged_df['GSADF_Stat'], color=color_bsadf_stat, label='Explosivity Test Statistic')\n",
    "ax3.tick_params(axis='y', labelcolor=axis_color)\n",
    "ax3.grid(False)  # Disable gridlines for the tertiary axis\n",
    "\n",
    "# Title and layout\n",
    "plt.title('Capital Stock, Output and Stock Market Bubbles over Time', color=axis_color, fontsize=18, fontweight='bold')\n",
    "fig.tight_layout()  # Otherwise the right y-label is slightly clipped\n",
    "\n",
    "# Add a legend to the plot\n",
    "lines = [line1, line2, line3]\n",
    "labels = [line.get_label() for line in lines]\n",
    "ax1.legend(lines, labels, loc='best')\n",
    "\n",
    "\n",
    "# Save the plot to the specified directory and file\n",
    "plt.savefig(f'/Users/philiplacava/Desktop/Asset-Price-Bubble-Detection/results/{select}_Index_Price_GDP_F.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
